# 数据处理: 应用程序和数据如何打交道?

我们开发者无论是从事服务端的开发, 还是客户端的开发, 和数据打交道是必不可少的

对于客户端来说, 从服务端读取到数据, 往往需要做缓存(内存缓存或者SQLite缓存), 甚至需要本地存储(文件或者SQLite)

对于服务器来说, 跟数据打交道的场景就更加丰富了, 除了数据库和缓存外, 还有大量的文本数据的索引(比如搜索引擎), 实时的消息队列对数据做流式处理, 或者非实时的批理对数据仓库(data wareouse)中的海量数据就进行ETL(Extract, Transfrom and Load)

![image-20241225164622329](assets/image-20241225164622329.png)

今天我们就来讲讲如何用Rust做数据处理, 主要讲两部分, 如何用Rust访问关系数据库, 以及如何用Rust对半结构化数据进行分析和处理, 希望通过学习这一讲的内容, 尤其是后半部分的内容, 能帮你打开眼界, 对数据处理有更加深刻的认识

## 访问关系数据库

作为互联网应用的最主要的数据存储和访问工具, 关系数据库, 是几乎没门编程语言都有良好支持的数据库类型

在Rust下, 有几乎所有主流关系数据库的驱动, 比如rust-postgres, rust-mysql-simple等, 不过一般我们不太会直接使用数据库的驱动来访问数据库, 因为那样会让应用过于耦合与某个数据库, 所以我们会使用ORM

Rust下有diesel这个非常成熟的ORM, 还有sea-orm这样的后起之秀, diesel不支持异步, 而sea-orm支持异步, 所以有理由相信, 随着sea-orm的不断程序, 会有越来越多的应用在sea-orm上构建

如果你觉得ORM太过笨重, 繁文缛节太多, 但又不想直接使用某个数据的驱动来访问数据, 那么你还可以用sqlx, sqlx提供了对多种数据的异步访问支持, 并且不使用DSL就可以对SQL query做编译时检查, 非常轻便; 它可以从数据库中直接查询出来一行数据, 也可以通过派生宏自行把行数据转换成对应的结构

今天, 我们就尝试使用sqlx处理用户注册和登陆两个非常常见的功能

## sqlx

构建下面的表结构来处理用户登陆信息:

```sql
CREATE TABLE IF NOT EXISTS users
(
    id INTEGER PRIMARY KEY NOT NULL,
    email VARCHAR UNIQUE NOT NULL,
    hashed_password VARCHAR NOT NULL
);
```

特别说明一下, 在数据库中存储用户信息需要非常谨慎, 尤其是涉及的敏感, 比如密码, 需要使用特定的哈希算法存储, OWASP对密码的存储有如下安全建议:

1. 若Argon2id可用, 那么使用Argon2id(需要目标及其至少有15MB内存)
2. 若Argon2id不可用, 那么使用bcrypt(算法至少迭代10次)
3. 之后再考虑scryp / PBKDF2

Argon2id是Argon2d和Argon2i的组合, Argon2d提供了强大的抗GPU破解能力, 但在特定的情况下容易遭受旁路攻击, 而Argon2i则可以防止旁路攻击, 但抗CPU破解稍弱, 所以只要是编程语言支持Argon2id, 那么它就是首选的密码哈希工具

Rust下有完善的password-hashes工具, 我们可以使用其中的argon2 crate, 用它来生成一个完整的, 包含所有参数的密码哈希长这个样子

```bash
$argon2id$v=19$m=4096,t=3,p=1$l7IEIWV7puJYJAZHyyut8A$OPxL09ODxp/xDQEnlG1NWdOsTr7RzuleBtiYQsnCyXY
```

这个字符串中包含了agon2id的版本(19), 使用的内存大小(4096k), 迭代次数(3次), 并行程度(1个线程), 以及base64编码的salt和hash

所以, 当新用户注册的时候, 我们使用argon2把传入的密码哈希一下, 存储到数据库中; 当用户使用email/password登录时, 我们通过email找到用户, 然后在通过argon2验证密码, 数据库的访问使用sqlx, 为了简单起见, 避免安装额外的数据, 就使用SQLite来存储数据(如果你本地有Mysql或者PostgreSQL, 可以自行替换相应的语句)

有了这个思路, 我们创建一个新项目, 添加相关的依赖

```toml
[package]
name = "_01_data_processing"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = "1.0.95"
argon2 = "0.5.3"
lazy_static = "1.5.0"
rand_core = { version = "0.6.4", features = ["std"] }
sqlx = { version = "0.8.2", features = ["runtime-tokio-rustls", "sqlite"] }
tokio = { version = "1.42.0", features = ["full"] }
```

然后创建`examples/user.rs`填入文件, 你可以对照详细的注释来理解:

```rust
use anyhow::{anyhow, Result};
use argon2::{password_hash::SaltString, Argon2, PasswordHash, PasswordHasher, PasswordVerifier};
use lazy_static::lazy_static;
use rand_core::OsRng;
use sqlx::{sqlite::SqlitePoolOptions, SqlitePool};
use std::env;

/// Argon2 hash使用的密码
const ARGON_SECRET: &[u8] = b"deadbeef";
lazy_static! {
  /// Argon2
  static ref ARGON2: Argon2<'static> = Argon2::new_with_secret(ARGON_SECRET, argon2::Algorithm::default(), argon2::Version::default(), argon2::Params::default()).unwrap();
}

/// user表对应的数据结构, 处理login/register
pub struct UserDb {
    poll: SqlitePool,
}

/// 使用FromRow派生宏从数据库中读取出来的数据结构转成User
#[allow(dead_code)]
#[derive(Debug, sqlx::FromRow)]
pub struct User {
    id: i64,
    email: String,
    hashed_password: String,
}

impl UserDb {
    pub fn new(poll: SqlitePool) -> Self {
        Self { poll }
    }

    /// 用户注册: 在users表中存储argon2哈希过的密码
    pub async fn regsiter(&self, email: &str, password: &str) -> Result<i64> {
        let hashed_password = generate_password_hash(password)?;

        let id = sqlx::query("INSERT INTO users(email, hashed_password) VALUES(?, ?)")
            .bind(email)
            .bind(hashed_password)
            .execute(&self.poll)
            .await?
            .last_insert_rowid();

        Ok(id)
    }

    /// 用户登陆: 从users表中获取用户信息, 并用验证密码
    pub async fn login(&self, email: &str, password: &str) -> Result<String> {
        let user: User = sqlx::query_as("SELECT * from users WHERE email = ?")
            .bind(email)
            .fetch_one(&self.poll)
            .await?;

        println!("fin user: {user:?}");

        if let Err(_) = verify_password(password, &user.hashed_password) {
            return Err(anyhow!("failed to login"));
        }

        // 生成JWT token(此处省略JWT token的生成的细节)
        Ok("awesome token".into())
    }
}

/// 冲洗你创建users表
async fn recreate_table(poll: &SqlitePool) -> Result<()> {
    sqlx::query("DROP TABLE users").execute(poll).await?;
    sqlx::query(
        r#"CREATE TABLE IF NOT EXISTS users(
            id      INTEGER     PRIMARY     KEY     NOT NULL,
            email   VARCHAR     UNIQUE      NOT NULL,
            HASHED_PASSWORD     VARCHAR     NOT NULL
        )"#,
    )
    .execute(poll)
    .await?;

    Ok(())
}

/// 创建安全的密码哈希
fn generate_password_hash(password: &str) -> Result<String> {
    let salt = SaltString::generate(&mut OsRng);
    let argon2 = Argon2::default();
    Ok(argon2
        .hash_password(password.as_bytes(), &salt)
        .map_err(|_| anyhow!("failed to hash password"))?
        .to_string())
}

/// 使用argon2验证用户密码和哈希
fn verify_password(password: &str, password_hash: &str) -> Result<()> {
    let parsed_hash =
        PasswordHash::new(password_hash).map_err(|_| anyhow!("failed to parse hashed password"))?;
    ARGON2
        .verify_password(password.as_bytes(), &parsed_hash)
        .map_err(|_| anyhow!("failed to verify password"))?;
    Ok(())
}

#[tokio::main]
async fn main() -> Result<()> {
    let url = env::var("DATBASE_URL").unwrap_or("sqlite:://./data/example.db".into());

    // 创建连接池
    let poll = SqlitePoolOptions::new()
        .max_connections(5)
        .connect(&url)
        .await?;

    // 每次运行都重新创建users表
    recreate_table(&poll).await?;

    let user_db = UserDb::new(poll.clone());
    let email = "nyh@163.com";
    let password = "nyh196511";

    // 新用户注册
    let id = user_db.regsiter(email, password).await?;
    println!("registered id: {id}");

    // 用户成功登陆
    let token = user_db.login(email, password).await?;
    println!("Login succeed: {token}");

    // 登陆失败
    let result = user_db.login(email, "dshdsiuchdsui").await;
    println!("Login should fail with bad password: {result:?}");

    Ok(())
}
```

在这段代码里, 我们把argon2的能力稍微包装一下, 提供了generate_password_hash和verify_password两个方法给注册和登陆使用, 对于数据库的访问, 我们提供了一个连接池SqlitePool, 便于无锁访问

你可能注意到了这句写法

```rust
let user: User = sqlx::query_as("SELECT * from users WHERE email = ?")
.bind(email)
.fetch_one(&self.pool)
.await?;
```

是不是很惊讶, 一般来说, 这是ORM才有的功能, 没错, 它再次体现了Rust trait的强大: 我们并不需要ORM就可以吧数据库中的数跟某个Model结合起来, 只需要在查询的时, 提供想要的转换成的数据结构`T: FromRow`即可

看query_as函数和FromRow trait的定义

```rust
pub fn query_as<'q, DB, O>(sql: &'q str) -> QueryAs<'q, DB, O, <DB as Database>::Arguments<'q>>
where
DB: Database,
O: for<'r> FromRow<'r, DB::Row>,
{
    QueryAs {
        inner: query(sql),
        output: PhantomData,
    }
}

pub trait FromRow<'r, R: Row>: Sized {
    fn from_row(row: &'r R) -> Result<Self, Error>;
}
```

希望这个例子可以让你体会到Rust处理数据库的强大和简约, 我们用Rust写出了Node.js / Python都不曾拥有的直观感受, 另外, sqlx是一个非常漂亮的crate, 有空的话建议你看看它的源码, 开头介绍的sea-orm, 底层也是使用sqlx

## 用Rust对半结构化数据进行分析

以生产环境为例, 我们会基类大量的半结构化数据, 比如各种各样的日志, 监控数据和分析数据

以日志为例, 虽然通常会将其灌入日志分析工具, 通过可视化界面进行分析和问题追踪, 但是偶尔我们也需要自己洗点小工具进行处理, 一般会用Python来完成这样的任务, 因为Python有pandas这样用起来非常舒服的工具, 然而pandas太吃内存了, 运算效率也不算高, 有没有更好的选择

我们之前介绍过polars, 也用polars和sqlparser写了一个处理csv的工具, 其实polars底层使用了Apache arrow, 如果你经常进行大数据处理, 那么你对列式存储和Data Frame应该比较熟悉, arrow就是一个在内存中进行存储和运算的列式存储, 它是构建下一代数据分析平台的基础软件

由于Rust在业界的地位越来越重要, Apache arrow也构建了完全用Rust实现的版本, 并在此基础上构建了高效的in-memeory查询伊宁datafusion, 以及在某些场景下可以去掉Spark的分布式查询引擎ballista

Apache arrow和datafusion目前已经有了很多重磅级的应用, 其中最令人兴奋的是InfluxDB IOx, 它是下一代InfluxDB的核心引擎

来一起感受一下datafusion如何使用

```rust
use datafusion::prelude::*;
use datafusion::arrow::util::pretty::print_batches;
use datafusion::arrow::record_batch::RecordBatch;

#[tokio::main]
async fn main() -> datafusion::error::Result<()> {
  // register the table
  let mut ctx = ExecutionContext::new();
  ctx.register_csv("example", "tests/example.csv", CsvReadOptions::new()).await?;

  // create a plan to run a SQL query
  let df = ctx.sql("SELECT a, MIN(b) FROM example GROUP BY a LIMIT 100").await?;

  // execute and print results
  df.show().await?;
  Ok(())
}
```

在这段代码中, 我们通过CsvReadOptions推断CSV的schema, 然后将其注册为一个逻辑上的example表, 之后通过SQL进行查询了, 是不是很强大

下面我们就使用datafusion, 来构建一个Nginx日志的命令行分析工具

## datasfusion

我们创建一个名为`nginx_logs.csv`的文件, 格式如下

```rust
93.180.71.3 - - "17/May/2015:08:05:32 +0000" GET "/downloads/product_1" "HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
93.180.71.3 - - "17/May/2015:08:05:23 +0000" GET "/downloads/product_1" "HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
80.91.33.133 - - "17/May/2015:08:05:24 +0000" GET "/downloads/product_1" "HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.17)"
```

这个日志共有十个域, 除了几个-外, 无法猜测是什么内容外, 其他的域都很好猜测

由于nginx_logs的格式是在Nginx配置中构建的, 所以日志文件, 并不像CSV文件那样有一行headeer, 没有header, 就无法让datafusion直接帮助我们推断出Schema, 也就是说我们需要显示的告诉datafusion日志文件的schema长什么样子

不过与datafusuion来说, 创建一个schema很简单, 比如

```rust
let schema = Arc::new(Schema::new(vec![
    Filed::new("ip", DataType::Utf8, false),
    Filed::new("code", DataType::Int32, false),
]))
```

为了最大的灵活性, 我们可以对应的构建一个简单的schema定义文件, 里面每个字段按顺序对应nginx日志的字段

```
---
- name: ip
type: string
- name: unused1
type: string
- name: unused2
type: string
- name: date
type: string
- name: method
type: string
- name: url
type: string
- name: version
type: string
- name: code
type: integer
- name: len
type: integer
- name: unused3
type: string
- name: ua
type: string
```

这样我们未来如果遇到不一样的日志文件, 我们可以修改schema的定义, 而无需修改程序本身

对于这个schema定义文件, 使用serde和serde-yaml来读取, 然后在实现From trait把SchemaField对应到的datafusion的Field结构:

```rust
use std::sync::Arc;

use datafusion::arrow::datatypes::{DataType, Field, Schema, SchemaRef};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord, Hash)]
#[serde(rename_all = "snake_case")]
pub enum SchemaDataType {
    /// Int64
    Integer,
    /// Utf8
    String,
    /// Date64
    Date,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
struct SchemaFiled {
    name: String,
    pub(crate) data_type: SchemaDataType,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
struct SchemaFields(Vec<SchemaFiled>);

impl From<SchemaDataType> for DataType {
    fn from(value: SchemaDataType) -> Self {
        match value {
            SchemaDataType::Integer => Self::Int64,
            SchemaDataType::String => Self::Utf8,
            SchemaDataType::Date => Self::Date64,
        }
    }
}

impl From<SchemaFiled> for Field {
    fn from(value: SchemaFiled) -> Self {
        Self::new(&value.name, value.data_type.into(), false)
    }
}

impl From<SchemaFields> for SchemaRef {
    fn from(value: SchemaFields) -> Self {
        let fields: Vec<Field> = value.0.into_iter().map(|f| f.into()).collect();

        Arc::new(Schema::new(fields))
    }


}
```

有了这个基本的schema转换的功能, 就可以构建我们的nginx的日志处理结构及其功能了

```rust
use std::sync::Arc;

use datafusion::{
    arrow::datatypes::{DataType, Field, Schema, SchemaRef},
    prelude::*,
};
use serde::{Deserialize, Serialize};

use anyhow::Result;

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord, Hash)]
#[serde(rename_all = "snake_case")]
pub enum SchemaDataType {
    /// Int64
    Integer,
    /// Utf8
    String,
    /// Date64
    Date,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
struct SchemaFiled {
    name: String,
    pub(crate) data_type: SchemaDataType,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
struct SchemaFields(Vec<SchemaFiled>);

impl From<SchemaDataType> for DataType {
    fn from(value: SchemaDataType) -> Self {
        match value {
            SchemaDataType::Integer => Self::Int64,
            SchemaDataType::String => Self::Utf8,
            SchemaDataType::Date => Self::Date64,
        }
    }
}

impl From<SchemaFiled> for Field {
    fn from(value: SchemaFiled) -> Self {
        Self::new(&value.name, value.data_type.into(), false)
    }
}

impl From<SchemaFields> for SchemaRef {
    fn from(value: SchemaFields) -> Self {
        let fields: Vec<Field> = value.0.into_iter().map(|f| f.into()).collect();

        Arc::new(Schema::new(fields))
    }
}

/// nginx日志处理的数据结构
pub struct NginxLog {
    ctx: SessionContext,
}

impl NginxLog {
    /// 根据schema定义, 数据文件以及分隔符构建NginxLog结构
    pub async fn try_new(schema_file: &str, data_file: &str, delim: u8) -> Result<Self> {
        let content = tokio::fs::read_to_string(schema_file).await?;
        let fields: SchemaFields = serde_yaml::from_str(&content)?;
        let schema = SchemaRef::from(fields);

        let mut ctx = SessionContext::new();
        let options = CsvReadOptions::new()
            .has_header(false)
            .delimiter(delim)
            .schema(&schema);
        ctx.register_csv("nginx", data_file, options).await?;

        Ok(Self { ctx })
    }
}
```

上面这段代码完成了nginx日志文件的读取, 解析和查询功能

再来写段代码来调用它:

```rust
#[tokio::main]
async fn main() -> Result<()> {
    println!("1");
    let mut nginx_log =
        NginxLog::try_new("fixtures/log_schema.yml", "fixtures/logs.csv", b' ').await?;
    println!("2");
    // 从stdin中按行读取, 当做sql查询进行处理
    let stdin = io::stdin();
    println!("3");

    let mut lines = stdin.lock().lines();
    println!("4");

    while let Some(Ok(line)) = lines.next() {
        println!("5");
        if !line.starts_with("--") {
            println!("6");
            println!("{line}");
            // 读到一行sql, 查询, 获取dataframe
            let df = nginx_log.query(&line).await?;
            df.show().await?;
        }
        println!("7");
    }
    println!("8");

    Ok(())
}
```

在这段代码里, 我们从stdin中获取内容, 把每一行输入都作为一个SQL语句传给nginx_log.query, 然后查询显示结果

来测试一下:

```bash
$ cargo run --example logs --quiet
SELECT * FROM nginx  
```

是不是很厉害? 我们可以充分利用SQL的强大表现力, 做各种复杂的查询, 不光如此, 还可以从一个包含了镀铬sql语句的文件中, 一次性做多个查询, 比如我们创建一个analyze.sql文件

```sql
-- 查询 ip 前 10 名
SELECT ip, count(*) as total, cast(avg(len) as int) as avg_len FROM nginx GROUP BY ip ORDER BY total DESC LIMIT 10
-- 查询 UA 前 10 名
select ua, count(*) as total from nginx group by ua order by total desc limit 10
-- 查询访问最多的 url 前 10 名
select url, count(*) as total from nginx group by url order by total desc limit 10
-- 查询访问返回 body 长度前 10 名
select len, count(*) as total from nginx group by len order by total desc limit 10
-- 查询 HEAD 请求
select ip, date, url, code, ua from nginx where method = 'HEAD' limit 10
-- 查询状态码是 403 的请求
select ip, date, url, ua from nginx where code = 403 limit 10
-- 查询 UA 为空的请求
select ip, date, url, code from nginx where ua = '-' limit 10
-- 复杂查询，找返回 body 长度的 percentile 在 0.5-0.7 之间的数据
select * from (select ip, date, url, ua, len, PERCENT_RANK() OVER (ORDER BY len) as len_percentile from nginx where code = 200 order by len desc) as t where t.len_percentile > 0.5 and t.len_percentile < 0.7 order by t.len_percentile desc limit 10
```

那么我们可以这样获取结果

```bash
❯ cat fixtures/analyze.sql | cargo run --example log --quiet
SELECT ip, count(*) as total, cast(avg(len) as int) as avg_len FROM nginx GROUP BY ip ORDER BY total DESC LIMIT 10
+-----------------+-------+---------+
| ip              | total | avg_len |
+-----------------+-------+---------+
| 216.46.173.126  | 2350  | 220     |
| 180.179.174.219 | 1720  | 292     |
| 204.77.168.241  | 1439  | 340     |
| 65.39.197.164   | 1365  | 241     |
| 80.91.33.133    | 1202  | 243     |
| 84.208.15.12    | 1120  | 197     |
| 74.125.60.158   | 1084  | 300     |
| 119.252.76.162  | 1064  | 281     |
| 79.136.114.202  | 628   | 280     |
| 54.207.57.55    | 532   | 289     |
+-----------------+-------+---------+
select ua, count(*) as total from nginx group by ua order by total desc limit 10
+-----------------------------------------------+-------+
| ua                                            | total |
+-----------------------------------------------+-------+
| Debian APT-HTTP/1.3 (1.0.1ubuntu2)            | 11830 |
| Debian APT-HTTP/1.3 (0.9.7.9)                 | 11365 |
| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21) | 6719  |
| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.16) | 5740  |
| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.22) | 3855  |
| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.17) | 1827  |
| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.7)  | 1255  |
| urlgrabber/3.9.1 yum/3.2.29                   | 792   |
| Debian APT-HTTP/1.3 (0.9.7.8)                 | 750   |
| urlgrabber/3.9.1 yum/3.4.3                    | 708   |
+-----------------------------------------------+-------+
select url, count(*) as total from nginx group by url order by total desc limit 10
+----------------------+-------+
| url                  | total |
+----------------------+-------+
| /downloads/product_1 | 30285 |
| /downloads/product_2 | 21104 |
| /downloads/product_3 | 73    |
+----------------------+-------+
select len, count(*) as total from nginx group by len order by total desc limit 10
+-----+-------+
| len | total |
+-----+-------+
| 0   | 13413 |
| 336 | 6652  |
| 333 | 3771  |
| 338 | 3393  |
| 337 | 3268  |
| 339 | 2999  |
| 331 | 2867  |
| 340 | 1629  |
| 334 | 1393  |
| 332 | 1240  |
+-----+-------+
select ip, date, url, code, ua from nginx where method = 'HEAD' limit 10
+----------------+----------------------------+----------------------+------+-------------------------+
| ip             | date                       | url                  | code | ua                      |
+----------------+----------------------------+----------------------+------+-------------------------+
| 184.173.149.15 | 23/May/2015:15:05:53 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:30 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:33 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:34 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:52 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:43 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:42 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:17:05:46 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |
| 5.153.24.140   | 23/May/2015:18:05:10 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |
| 184.173.149.16 | 24/May/2015:18:05:37 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |
+----------------+----------------------------+----------------------+------+-------------------------+
select ip, date, url, ua from nginx where code = 403 limit 10
+----------------+----------------------------+----------------------+-----------------------------------------------------------------------------------------------------+
| ip             | date                       | url                  | ua                                                                                                  |
+----------------+----------------------------+----------------------+-----------------------------------------------------------------------------------------------------+
| 184.173.149.15 | 23/May/2015:15:05:53 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 5.153.24.140   | 23/May/2015:17:05:33 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 5.153.24.140   | 23/May/2015:17:05:34 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 184.173.149.16 | 24/May/2015:18:05:37 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 195.88.195.153 | 24/May/2015:23:05:05 +0000 | /downloads/product_2 | curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3 |
| 184.173.149.15 | 25/May/2015:04:05:14 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 87.85.173.82   | 17/May/2015:14:05:07 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 87.85.173.82   | 17/May/2015:14:05:11 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 194.76.107.17  | 17/May/2015:16:05:50 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
| 194.76.107.17  | 17/May/2015:17:05:40 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |
+----------------+----------------------------+----------------------+-----------------------------------------------------------------------------------------------------+
select ip, date, url, code from nginx where ua = '-' limit 10
+----------------+----------------------------+----------------------+------+
| ip             | date                       | url                  | code |
+----------------+----------------------------+----------------------+------+
| 217.168.17.150 | 01/Jun/2015:14:06:45 +0000 | /downloads/product_2 | 200  |
| 217.168.17.180 | 01/Jun/2015:14:06:15 +0000 | /downloads/product_2 | 200  |
| 217.168.17.150 | 01/Jun/2015:14:06:18 +0000 | /downloads/product_1 | 200  |
| 204.197.211.70 | 24/May/2015:06:05:02 +0000 | /downloads/product_2 | 200  |
| 91.74.184.74   | 29/May/2015:14:05:17 +0000 | /downloads/product_2 | 403  |
| 91.74.184.74   | 29/May/2015:15:05:43 +0000 | /downloads/product_2 | 403  |
| 91.74.184.74   | 29/May/2015:22:05:53 +0000 | /downloads/product_2 | 403  |
| 217.168.17.5   | 31/May/2015:02:05:16 +0000 | /downloads/product_2 | 200  |
| 217.168.17.180 | 20/May/2015:23:05:22 +0000 | /downloads/product_2 | 200  |
| 204.197.211.70 | 21/May/2015:02:05:34 +0000 | /downloads/product_2 | 200  |
+----------------+----------------------------+----------------------+------+
select * from (select ip, date, url, ua, len, PERCENT_RANK() OVER (ORDER BY len) as len_percentile from nginx where code = 200 order by len desc) as t where t.len_percentile > 0.5 and t.len_percentile < 0.7 order by t.len_percentile desc limit 10
+----------------+----------------------------+----------------------+-----------------------------+------+--------------------+
| ip             | date                       | url                  | ua                          | len  | len_percentile     |
+----------------+----------------------------+----------------------+-----------------------------+------+--------------------+
| 54.229.83.18   | 26/May/2015:00:05:34 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |
| 54.244.37.198  | 18/May/2015:10:05:39 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |
| 67.132.206.254 | 29/May/2015:07:05:52 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |
| 128.199.60.184 | 24/May/2015:00:05:09 +0000 | /downloads/product_1 | urlgrabber/3.10 yum/3.4.3   | 2592 | 0.6342190216041719 |
| 54.173.6.142   | 27/May/2015:14:05:21 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |
| 104.156.250.12 | 03/Jun/2015:11:06:51 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |
| 115.198.47.126 | 25/May/2015:11:05:13 +0000 | /downloads/product_1 | urlgrabber/3.10 yum/3.4.3   | 2592 | 0.6342190216041719 |
| 198.105.198.4  | 29/May/2015:07:05:34 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |
| 107.23.164.80  | 31/May/2015:09:05:34 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |
| 108.61.251.29  | 31/May/2015:10:05:16 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |
+----------------+----------------------------+----------------------+-----------------------------+------+--------------------+
```

## 小结

我们今天介绍了如何使用Rust处理存放在关系数据库中的结构化数据, 以及我们存放在系统文件中的半结构化数据

虽然在工作汇总, 我们不太会使用arrow/datafusion去创建某一个下一代的数据处理平台, 但拥有了处理半结构化数据的能力, 可以解决很多实际的问题

比如每个10分钟扫描Nginx/CDN, 以及应用服务器过去了10分钟的日志, 找到某些非正常的访问, 然后把该用户/设备的访问切断一阵子的特殊需求, 一般的数据平台很难处理, 需要我们自己来撰写代码实现, 此时arrow/datafusion这样的工具就很方便

